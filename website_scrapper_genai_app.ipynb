{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Langchain workflow, I loaded the data into a vectordb (FAISS) and created the retrieval and document chains to help LLM(GPT-4o) model to gather the query and provide the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapped and load the data from web\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "page_url = \"https://python.langchain.com/docs/how_to/#chat-models\"\n",
    "loader = WebBaseLoader(page_url)\n",
    "\n",
    "doc=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting/Chunking the data and saving to vectordb after converting to vector embeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n",
    "# Splitting documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "split_doc = text_splitter.split_documents(doc)\n",
    "\n",
    "# Convert to a list of Document objects\n",
    "document_list = [Document(chunk.page_content, metadata=chunk.metadata) for chunk in split_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.vectorstores.faiss.FAISS"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Created the embeddings from Open Ai embeddings library and used FAISS for vector db\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_storedb = FAISS.from_documents(document_list, embeddings)\n",
    "\n",
    "type(vector_storedb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to: cache model responses\\nHow to: create a custom LLM class\\nHow to: stream a response back\\nHow to: track token usage\\nHow to: work with local models\\n\\nOutput parsers\\u200b\\nOutput Parsers are responsible for taking the output of an LLM and parsing into more structured format.\\n\\nHow to: parse text from message objects\\nHow to: use output parsers to parse an LLM response into structured format\\nHow to: parse JSON output\\nHow to: parse XML output\\nHow to: parse YAML output\\nHow to: retry when output parsing errors occur\\nHow to: try to fix errors in output parsing\\nHow to: write a custom output parser class\\n\\nDocument loaders\\u200b\\nDocument Loaders are responsible for loading documents from a variety of sources.\\n\\nHow to: load PDF files\\nHow to: load web pages\\nHow to: load CSV data\\nHow to: load data from a directory\\nHow to: load HTML data\\nHow to: load JSON data\\nHow to: load Markdown data\\nHow to: load Microsoft Office data\\nHow to: write a custom document loader'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Querying the vectordb\n",
    "results = vector_storedb.similarity_search(query='Document Loaders are responsible for what?')\n",
    "results[0].page_content #Most relevent response based on the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the questions based only on the provided context:\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x11fd2ab90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x11fd95d80>, root_client=<openai.OpenAI object at 0x11fd67880>, root_async_client=<openai.AsyncOpenAI object at 0x11fd2b400>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document chain \n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain #Create chains for passing a list of documents to a model\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(  \n",
    "\"\"\"\n",
    "Answer the questions based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    ")     ## Here we can provide \n",
    "\n",
    "document_chain.invoke({\n",
    "\"input\":\"Document Loaders are responsible for what?\",\n",
    "\"context\":[Document(page_content = \"Document loaders\\nDocument Loaders are responsible for loading documents from a variety of sources.\")]\n",
    "}) #Here we passed documents are context for our question\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "# The context is provided to the above prompt using create_stuff_documents_chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x11b051420>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the questions based only on the provided context:\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x11fd2ab90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x11fd95d80>, root_client=<openai.OpenAI object at 0x11fd67880>, root_async_client=<openai.AsyncOpenAI object at 0x11fd2b400>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retrieval chain: \n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrive_db = vector_storedb.as_retriever() \n",
    "retrivel_chain = create_retrieval_chain(retrive_db, document_chain) # Here we create a chain between the vector db and the context to the llm\n",
    "retrivel_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = retrivel_chain.invoke({'input':'Document Loaders are responsible for what?'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. What are Output Parsers responsible for?\\n\\n   Output Parsers are responsible for taking the output of an LLM and parsing it into a more structured format.\\n\\n2. How can you parse JSON output according to the context?\\n\\n   You can parse JSON output by using output parsers designed to handle JSON data.\\n\\n3. What methods are mentioned for selecting examples?\\n\\n   Methods for selecting examples mentioned in the context include selecting examples by length, maximal marginal relevance (MMR), n-gram overlap, and similarity.\\n\\n4. What is the purpose of Document Loaders?\\n\\n   Document Loaders are responsible for loading documents from a variety of sources.\\n\\n5. How can LLM responses be cached?\\n\\n   LLM responses can be cached, as indicated by the context mentioning \"How to cache model responses\" and \"How to cache chat model responses.\"'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
